{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import sys"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(sys.version)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "import os\n", "import re\n", "import shutil\n", "import string"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import tensorflow as tf"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Tensorflow version: {}\".format(tf.__version__))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Using regex (or at least Tensorflow's implementation of it) to prase the strings and remove break statements, punctuation, and convert all to lowercase"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def custom_standardization(in_data):\n", "    lowercase = tf.strings.lower(in_data)\n", "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n", "    return tf.strings.regex_replace(\n", "        stripped_html, \"[%s]\" % re.escape(string.punctuation), \"\"\n", "    )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This uses Tensorflow's functions of the TextVectorization layer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def vectorize_text(text, label):\n", "    text = tf.expand_dims(text, -1)\n", "    return vectorize_layer(text), label"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Grabbing the dataset using a function of Tensorflow's"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n", "dataset = tf.keras.utils.get_file(\n", "    \"aclImdb_v1\", url, untar=True, cache_dir=\".\", cache_subdir=\"\"\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now that the dataset's been downloaded, using OS based function to navigate to its location"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dataset = \"./aclImdb\"\n", "print(type(dataset))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dataset_dir = os.path.join(os.path.dirname(dataset), \"aclImdb\")\n", "train_dir = os.path.join(dataset_dir, \"train\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Checking out the contents of a sample file"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sample_file = os.path.join(train_dir, \"pos/1181_9.txt\")\n", "with open(sample_file) as f:\n", "    print(f.read())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Removing this directory because Tensorflow's text_dataset_from_directory fuction is based on the directories"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["try:\n", "    remove_dir = os.path.join(train_dir, \"unsup\")\n", "    shutil.rmtree(remove_dir)\n", "    print(\"File removed.\")\n", "except FileNotFoundError:\n", "    print(\"File no longer exists.\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Set here because they're used for each dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["batch_size = 32\n", "seed = 42"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Using Tensorflow's function to grab a dataset from each directory. Training dataset split into training and validation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n", "    \"aclImdb/train\",\n", "    batch_size=batch_size,\n", "    validation_split=0.2,\n", "    subset=\"training\",\n", "    seed=seed,\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n", "    \"aclImdb/train\",\n", "    batch_size=batch_size,\n", "    validation_split=0.2,\n", "    subset=\"validation\",\n", "    seed=seed,\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n", "    \"aclImdb/test\", batch_size=batch_size\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Grabbing the first batch, and printing the first three reviews and labels."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for text_batch, label_batch in raw_train_ds.take(1):\n", "    for i in range(3):\n", "        print(\"Review:\", text_batch.numpy()[i])\n", "        print(\"Label:\", label_batch.numpy()[i])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Printing out label correspondance. From the names of the directory"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n", "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Variables used in vectorization"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sequence_length = 300\n", "max_features = 10000"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Creating one of Tensorflow's TextVectorization layers to vectorize our reviews"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["vectorize_layer = tf.keras.layers.TextVectorization(\n", "    standardize=custom_standardization,\n", "    max_tokens=max_features,\n", "    output_mode=\"int\",\n", "    output_sequence_length=sequence_length,\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Passing through the anonymous lambda funcion, that will take x and y as inputs then return x. Kinds similar to an arrow function in JS"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_text = raw_train_ds.map(lambda x, y: x)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["After setting everything up we call Tensorflow's map function which converts strings to integers"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["vectorize_layer.adapt(train_text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Printing out a batch from the dataset. 32 reviews and labels."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["text_batch, label_batch = next(iter(raw_train_ds))\n", "review1, label1 = text_batch[0], label_batch[0]\n", "print(\"Review: {}\".format(review1))\n", "print(\"Label: {}\".format(raw_train_ds.class_names[label1]))\n", "print(\"Vectorized: {}\".format(vectorize_text(review1, label1)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Each string/word has been vectorized to an integer, so we're seeing what string corresponds to the integer 1287..."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"1287 --> {}\".format(vectorize_layer.get_vocabulary()[1287]))\n", "print(\"313 --> {}\".format(vectorize_layer.get_vocabulary()[313]))\n", "print(\"Vocabulary size: {}\".format(len(vectorize_layer.get_vocabulary())))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Mapping this vectorized text into our dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_ds = raw_train_ds.map(vectorize_text)\n", "val_ds = raw_val_ds.map(vectorize_text)\n", "test_ds = raw_test_ds.map(vectorize_text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Optimizing the eventual traning so that the dataset isn't a bottleneck"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["AUTOTUNE = tf.data.AUTOTUNE\n", "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n", "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n", "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Setting up the model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["embedding_dim = 16\n", "model = tf.keras.Sequential(\n", "    [\n", "        # This layer takes the integer-encoded reviews and looks up an embedding vector for each word-index. These vectors are learned as the model trains.\n", "        tf.keras.layers.Embedding(max_features + 1, embedding_dim),\n", "        tf.keras.layers.Dropout(0.2),\n", "        # This layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.\n", "        tf.keras.layers.GlobalAveragePooling1D(),\n", "        tf.keras.layers.Dropout(0.2),\n", "        # Last layer densely connected to a singular output node\n", "        tf.keras.layers.Dense(1),\n", "    ]\n", ")\n", "print(model.summary())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Setting optimizer and loss functions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model.compile(\n", "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n", "    optimizer=\"adam\",\n", "    metrics=tf.metrics.BinaryAccuracy(threshold=0.0),\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["model.fit returns a history object that contains a dictionary with everything that happened during training."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["epochs = 10\n", "history = model.fit(train_ds, validation_data=val_ds, epochs=epochs)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Again just getting loss and accuracy"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["loss, accuracy = model.evaluate(test_ds)\n", "print(\"Loss: {}\".format(loss))\n", "print(\"Accuracy: {}\".format(accuracy))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Accessing the history dict from the model.fit function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["history_dict = history.history\n", "print(history_dict.keys())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Grabbing the information from the dictionary"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["acc = history_dict[\"binary_accuracy\"]\n", "val_acc = history_dict[\"val_binary_accuracy\"]\n", "loss = history_dict[\"loss\"]\n", "val_loss = history_dict[\"val_loss\"]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Changing epoch to a range so that it can be plotted"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(epochs)\n", "epochs = range(1, len(acc) + 1)\n", "print(epochs)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plotting the training and validation losses"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(epochs, loss, \"bo\", label=\"Training Loss\")\n", "plt.plot(epochs, val_loss, \"b\", label=\"Validation Loss\")\n", "plt.title(\"Training and Validation Losses\")\n", "plt.xlabel(\"Epochs\")\n", "plt.ylabel(\"Loss\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Graphing the training and validation accuracy. The validation accuracy can be seen plateauing much sooner than the training accuracy which is an example of overfitting"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(epochs, acc, \"bo\", label=\"Training Accuracy\")\n", "plt.plot(epochs, val_acc, \"b\", label=\"Validation Accuracy\")\n", "plt.title(\"Training and Validation Accuracy\")\n", "plt.xlabel(\"Epochs\")\n", "plt.ylabel(\"Accuracy\")\n", "plt.legend(loc=\"lower right\")\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["export_model = tf.keras.Sequential(\n", "    [vectorize_layer, model, tf.keras.layers.Activation(\"sigmoid\")]\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["export_model.compile(\n", "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n", "    optimizer=\"adam\",\n", "    metrics=[\"accuracy\"],\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["loss, accuracy = export_model.evaluate(raw_test_ds)\n", "print(accuracy)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["examples = [\n", "    \"The movie was great!\",\n", "    \"The movie was okay.\",\n", "    \"The movie was terrible...\",\n", "]\n", "print(export_model.predict(examples))"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}